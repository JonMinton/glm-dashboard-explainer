[
  {
    "objectID": "claude.html",
    "href": "claude.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "claude.html#project-vision",
    "href": "claude.html#project-vision",
    "title": "",
    "section": "Project Vision",
    "text": "Project Vision\nBuild a guided, step-by-step interactive dashboard that helps learners understand: 1. How different data types require different model families 2. The role of link functions in connecting linear predictors to response constraints 3. How maximum likelihood estimation â€œtunesâ€ model parameters 4. The distinction between systematic (Î²) and random (Î±) components"
  },
  {
    "objectID": "claude.html#core-pedagogical-framework",
    "href": "claude.html#core-pedagogical-framework",
    "title": "",
    "section": "Core Pedagogical Framework",
    "text": "Core Pedagogical Framework\nThe GLM is presented as a data flow:\n[Xâ‚] â”€hâ‚()â†’ â”\n[Xâ‚‚] â”€hâ‚‚()â†’ â”œâ”€â†’ [Î· = XÎ²] â”€gâ»Â¹()â†’ [Î¼] â”€f(Â·;Î¼,Î±)â†’ [Y]\n[Xâ‚ƒ] â”€hâ‚ƒ()â†’ â”˜                              â†“\n                                      [y] (observed)\nWhere: - h() = Predictor transformers (log, sqrt, polynomial) - Î· = XÎ² = Linear predictor - gâ»Â¹() = Inverse link function (logitâ»Â¹, exp, identity) - Î¼ = Expected value of Y - f() = Distribution family (Binomial, Poisson, Gaussian, etc.) - Y = Random variable (what the model says could happen) - y = Observed data (what actually happened)"
  },
  {
    "objectID": "claude.html#key-distinctions-to-maintain",
    "href": "claude.html#key-distinctions-to-maintain",
    "title": "",
    "section": "Key Distinctions to Maintain",
    "text": "Key Distinctions to Maintain\n\nY vs y: Y is the modelled random variable; y is observed data\nÎ² vs Î±: Î² tunes the systematic component (via XÎ² â†’ Î¼); Î± tunes dispersion in f()\nLink vs Distribution: These are separate choices, though often paired conventionally"
  },
  {
    "objectID": "claude.html#page-structure-guided-flow",
    "href": "claude.html#page-structure-guided-flow",
    "title": "",
    "section": "Page Structure (Guided Flow)",
    "text": "Page Structure (Guided Flow)\n\nScenario Selection - Choose a data problem type (binary, count, continuous, etc.)\nData Exploration - Paginated table view of the dataset\nModel Family Selection - Choose distribution and link function\nPredictor Transformations - Apply h() transforms to X variables\nModel Fitting - Manual slider exploration then algorithmic optimization\nAdvanced: Loss Function Comparison - Compare MLE to other loss functions"
  },
  {
    "objectID": "claude.html#tech-stack",
    "href": "claude.html#tech-stack",
    "title": "",
    "section": "Tech Stack",
    "text": "Tech Stack\nPrimary: Quarto site (consistent with JonStats workflow)\n\nStructure: Quarto pages with guided navigation\nCode examples: R and Python for follow-along exercises\nInteractive visualizations: Observable JS for simpler interactions, React/custom JS for complex components (flow diagram, parameter sliders)\nHosting: GitHub Pages (static site)\nGLM fitting: Pre-computed in R during build, or WebR for in-browser computation where needed"
  },
  {
    "objectID": "claude.html#scenarios-to-implement",
    "href": "claude.html#scenarios-to-implement",
    "title": "",
    "section": "Scenarios to Implement",
    "text": "Scenarios to Implement\nPriority order (MVP first): 1. Hospital Readmissions (Binary/Logistic) - PRIMARY MVP 2. Insurance Claims (Count/Poisson) 3. Equipment Lifespan (Positive continuous/Gamma) 4. Crop Yields (Continuous/Gaussian)\nLater expansion: - Species Abundance (Zero-inflated) - Market Survey (Ordinal)"
  },
  {
    "objectID": "claude.html#development-principles",
    "href": "claude.html#development-principles",
    "title": "",
    "section": "Development Principles",
    "text": "Development Principles\n\nOne concept per page - Donâ€™t mix link functions and loss functions\nConcrete before abstract - See data before equations\nActive before passive - Manipulate before optimize\nBuild intuition before automation - Manual sliders before algorithms\nProgressive disclosure - Beginner/Intermediate/Advanced modes"
  },
  {
    "objectID": "claude.html#key-resources",
    "href": "claude.html#key-resources",
    "title": "",
    "section": "Key Resources",
    "text": "Key Resources\n\nJonStats Course Materials\nPrior discussion: support/claude-concept-chat.md"
  },
  {
    "objectID": "claude.html#file-structure",
    "href": "claude.html#file-structure",
    "title": "",
    "section": "File Structure",
    "text": "File Structure\nglm-dashboard-explainer/\nâ”œâ”€â”€ claude.md                 # This file - project overview\nâ”œâ”€â”€ _quarto.yml               # Quarto project configuration\nâ”œâ”€â”€ .claude/\nâ”‚   â”œâ”€â”€ settings.json         # Claude Code settings\nâ”‚   â”œâ”€â”€ architecture.md       # Architectural decisions (ADRs)\nâ”‚   â”œâ”€â”€ components.md         # Component specifications\nâ”‚   â””â”€â”€ commands/             # Custom slash commands\nâ”‚       â”œâ”€â”€ context.md        # Load full project context\nâ”‚       â”œâ”€â”€ review-page.md    # Review page implementation\nâ”‚       â”œâ”€â”€ add-scenario.md   # Add new scenario\nâ”‚       â””â”€â”€ check-notation.md # Check notation consistency\nâ”œâ”€â”€ support/\nâ”‚   â””â”€â”€ claude-concept-chat.md  # Prior discussion context\nâ”œâ”€â”€ pages/                    # Quarto pages (the actual content)\nâ”‚   â”œâ”€â”€ index.qmd             # Landing / scenario selection\nâ”‚   â”œâ”€â”€ data.qmd              # Data exploration page\nâ”‚   â”œâ”€â”€ model.qmd             # Model builder page\nâ”‚   â”œâ”€â”€ transform.qmd         # Predictor transformations\nâ”‚   â”œâ”€â”€ fit.qmd               # Fitting page\nâ”‚   â””â”€â”€ advanced/             # Advanced topics\nâ”‚       â””â”€â”€ loss-functions.qmd\nâ”œâ”€â”€ components/               # Reusable JS/Observable components\nâ”œâ”€â”€ data/                     # Sample datasets (R/CSV)\nâ”œâ”€â”€ R/                        # R scripts for data generation, examples\nâ””â”€â”€ _site/                    # Built site (git-ignored)"
  },
  {
    "objectID": "pages/model-builder.html",
    "href": "pages/model-builder.html",
    "title": "Build Your Model",
    "section": "",
    "text": "Code\n// Load dataset metadata\ndatasetMeta = FileAttachment(\"../data/heart.json\").json()\nCode\nmutable selectedPredictors = []\nmutable selectedResponse = null\nCode\navailableVars = {\n  const assigned = new Set([\n    ...selectedPredictors.map(p =&gt; p.name),\n    selectedResponse?.name\n  ].filter(Boolean))\n  return datasetMeta.variables.filter(v =&gt; !assigned.has(v.name))\n}\n\n// Type icons\ntypeIcon = (type) =&gt; {\n  switch(type) {\n    case \"continuous\": return \"ğŸ“Š\"\n    case \"binary\": return \"âš«âšª\"\n    case \"categorical\": return \"ğŸ“‘\"\n    case \"count\": return \"ğŸ”¢\"\n    default: return \"â“\"\n  }\n}"
  },
  {
    "objectID": "pages/model-builder.html#dataset-datasetmeta.name",
    "href": "pages/model-builder.html#dataset-datasetmeta.name",
    "title": "Build Your Model",
    "section": "Dataset: ",
    "text": "Dataset: \n\n\n\n\n\n\n\nHow to use this page\n\n\n\n\nDrag variables from the table below to assign them as predictors (left) or response (right)\nYou need at least one predictor and exactly one response to continue\nClick the âœ• on any assigned variable to return it to the pool\n\n\n\n\nAvailable Variables\nDrag variables from here to the model canvas below.\n\n\nCode\n// Variable card component\nviewof variableCards = {\n  const container = html`&lt;div class=\"variable-pool\"&gt;&lt;/div&gt;`\n\n  function render() {\n    container.innerHTML = ''\n\n    if (availableVars.length === 0) {\n      container.innerHTML = '&lt;p class=\"empty-message\"&gt;All variables assigned to model&lt;/p&gt;'\n      return\n    }\n\n    for (const v of availableVars) {\n      const card = html`\n        &lt;div class=\"variable-card\" draggable=\"true\" data-var=\"${v.name}\"&gt;\n          &lt;div class=\"var-header\"&gt;\n            &lt;span class=\"var-icon\"&gt;${typeIcon(v.type)}&lt;/span&gt;\n            &lt;span class=\"var-name\"&gt;${v.label}&lt;/span&gt;\n          &lt;/div&gt;\n          &lt;div class=\"var-meta\"&gt;\n            &lt;span class=\"var-type\"&gt;${v.type}&lt;/span&gt;\n            ${v.unit ? html`&lt;span class=\"var-unit\"&gt;(${v.unit})&lt;/span&gt;` : ''}\n          &lt;/div&gt;\n          &lt;div class=\"var-desc\"&gt;${v.description}&lt;/div&gt;\n        &lt;/div&gt;\n      `\n\n      card.addEventListener('dragstart', (e) =&gt; {\n        e.dataTransfer.setData('text/plain', v.name)\n        e.dataTransfer.effectAllowed = 'move'\n        card.classList.add('dragging')\n      })\n\n      card.addEventListener('dragend', () =&gt; {\n        card.classList.remove('dragging')\n      })\n\n      container.appendChild(card)\n    }\n  }\n\n  render()\n  return container\n}\n\n\n\n\n\n\n\n\n\n\nModel Canvas\n\n\nCode\nviewof predictorZone = {\n  const zone = html`\n    &lt;div class=\"drop-zone predictor-zone\"&gt;\n      &lt;h4&gt;Predictors (X)&lt;/h4&gt;\n      &lt;p class=\"zone-hint\"&gt;Variables that might explain or predict the response&lt;/p&gt;\n      &lt;div class=\"assigned-vars\" id=\"predictor-list\"&gt;&lt;/div&gt;\n    &lt;/div&gt;\n  `\n\n  const list = zone.querySelector('#predictor-list')\n\n  function renderPredictors() {\n    list.innerHTML = ''\n\n    if (selectedPredictors.length === 0) {\n      list.innerHTML = '&lt;p class=\"empty-hint\"&gt;Drop variables here&lt;/p&gt;'\n      return\n    }\n\n    selectedPredictors.forEach((v, i) =&gt; {\n      const chip = html`\n        &lt;div class=\"assigned-chip predictor-chip\"&gt;\n          &lt;span class=\"chip-label\"&gt;X&lt;sub&gt;${i + 1}&lt;/sub&gt;&lt;/span&gt;\n          &lt;span class=\"chip-icon\"&gt;${typeIcon(v.type)}&lt;/span&gt;\n          &lt;span class=\"chip-name\"&gt;${v.label}&lt;/span&gt;\n          &lt;button class=\"chip-remove\" data-var=\"${v.name}\"&gt;âœ•&lt;/button&gt;\n        &lt;/div&gt;\n      `\n\n      chip.querySelector('.chip-remove').addEventListener('click', () =&gt; {\n        mutable selectedPredictors = selectedPredictors.filter(p =&gt; p.name !== v.name)\n        renderPredictors()\n      })\n\n      list.appendChild(chip)\n    })\n  }\n\n  zone.addEventListener('dragover', (e) =&gt; {\n    e.preventDefault()\n    e.dataTransfer.dropEffect = 'move'\n    zone.classList.add('drag-over')\n  })\n\n  zone.addEventListener('dragleave', () =&gt; {\n    zone.classList.remove('drag-over')\n  })\n\n  zone.addEventListener('drop', (e) =&gt; {\n    e.preventDefault()\n    zone.classList.remove('drag-over')\n    const varName = e.dataTransfer.getData('text/plain')\n    const variable = datasetMeta.variables.find(v =&gt; v.name === varName)\n\n    if (variable && !selectedPredictors.find(p =&gt; p.name === varName)) {\n      // If it was the response, remove it from there\n      if (selectedResponse?.name === varName) {\n        mutable selectedResponse = null\n      }\n      mutable selectedPredictors = [...selectedPredictors, variable]\n      renderPredictors()\n    }\n  })\n\n  renderPredictors()\n  return zone\n}\n\n// Response drop zone\nviewof responseZone = {\n  const zone = html`\n    &lt;div class=\"drop-zone response-zone\"&gt;\n      &lt;h4&gt;Response (y)&lt;/h4&gt;\n      &lt;p class=\"zone-hint\"&gt;The outcome variable you want to model&lt;/p&gt;\n      &lt;div class=\"assigned-vars\" id=\"response-slot\"&gt;&lt;/div&gt;\n    &lt;/div&gt;\n  `\n\n  const slot = zone.querySelector('#response-slot')\n\n  function renderResponse() {\n    slot.innerHTML = ''\n\n    if (!selectedResponse) {\n      slot.innerHTML = '&lt;p class=\"empty-hint\"&gt;Drop one variable here&lt;/p&gt;'\n      return\n    }\n\n    const chip = html`\n      &lt;div class=\"assigned-chip response-chip\"&gt;\n        &lt;span class=\"chip-label\"&gt;y&lt;/span&gt;\n        &lt;span class=\"chip-icon\"&gt;${typeIcon(selectedResponse.type)}&lt;/span&gt;\n        &lt;span class=\"chip-name\"&gt;${selectedResponse.label}&lt;/span&gt;\n        &lt;button class=\"chip-remove\"&gt;âœ•&lt;/button&gt;\n      &lt;/div&gt;\n    `\n\n    chip.querySelector('.chip-remove').addEventListener('click', () =&gt; {\n      mutable selectedResponse = null\n      renderResponse()\n    })\n\n    slot.appendChild(chip)\n  }\n\n  zone.addEventListener('dragover', (e) =&gt; {\n    e.preventDefault()\n    e.dataTransfer.dropEffect = 'move'\n    zone.classList.add('drag-over')\n  })\n\n  zone.addEventListener('dragleave', () =&gt; {\n    zone.classList.remove('drag-over')\n  })\n\n  zone.addEventListener('drop', (e) =&gt; {\n    e.preventDefault()\n    zone.classList.remove('drag-over')\n    const varName = e.dataTransfer.getData('text/plain')\n    const variable = datasetMeta.variables.find(v =&gt; v.name === varName)\n\n    if (variable) {\n      // Remove from predictors if it was there\n      mutable selectedPredictors = selectedPredictors.filter(p =&gt; p.name !== varName)\n      mutable selectedResponse = variable\n      renderResponse()\n    }\n  })\n\n  renderResponse()\n  return zone\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCurrent Model Specification\n\n\nCode\nmodelSummary = {\n  if (!selectedResponse) {\n    return html`&lt;p class=\"model-incomplete\"&gt;âš ï¸ Select a response variable (y) to define your model&lt;/p&gt;`\n  }\n\n  if (selectedPredictors.length === 0) {\n    return html`&lt;p class=\"model-incomplete\"&gt;âš ï¸ Select at least one predictor variable (X)&lt;/p&gt;`\n  }\n\n  const predictorTerms = selectedPredictors.map((p, i) =&gt; `Î²&lt;sub&gt;${i+1}&lt;/sub&gt;Â·${p.label}`).join(' + ')\n\n  return html`\n    &lt;div class=\"model-equation\"&gt;\n      &lt;p&gt;&lt;strong&gt;Response:&lt;/strong&gt; ${selectedResponse.label} (${selectedResponse.type})&lt;/p&gt;\n      &lt;p&gt;&lt;strong&gt;Linear predictor:&lt;/strong&gt;&lt;/p&gt;\n      &lt;p class=\"equation\"&gt;Î· = Î²&lt;sub&gt;0&lt;/sub&gt; + ${predictorTerms}&lt;/p&gt;\n      &lt;p class=\"model-note\"&gt;\n        ${selectedResponse.type === 'continuous'\n          ? 'â†’ Gaussian family with identity link is a natural starting point'\n          : selectedResponse.type === 'binary'\n            ? 'â†’ Binomial family with logit link is appropriate'\n            : selectedResponse.type === 'count'\n              ? 'â†’ Poisson family with log link is a starting point'\n              : 'â†’ Consider the response type when choosing distribution family'}\n      &lt;/p&gt;\n    &lt;/div&gt;\n  `\n}\n\n\n\n\n\n\n\n\n\n\nCode\ncanProceed = selectedResponse !== null && selectedPredictors.length &gt; 0\n\nnavButtons = html`\n  &lt;div class=\"nav-buttons\"&gt;\n    &lt;a href=\"index.html\" class=\"btn btn-secondary\"&gt;â† Back to Scenarios&lt;/a&gt;\n    &lt;button class=\"btn btn-primary\" ${canProceed ? '' : 'disabled'}&gt;\n      Continue to Model Selection â†’\n    &lt;/button&gt;\n  &lt;/div&gt;\n`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSample Data Preview\n\n\nCode\n// Show first few rows of sample data\nInputs.table(datasetMeta.sampleData, {\n  columns: datasetMeta.variables.map(v =&gt; v.name),\n  header: Object.fromEntries(datasetMeta.variables.map(v =&gt; [v.name, v.label])),\n  width: \"100%\",\n  rows: 8\n})"
  },
  {
    "objectID": "background.html",
    "href": "background.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\nHello Claude!\nThis github connected project is intended to help develop and implement ideas for building a pedagogic website/dashboard for explaining key concepts in generalised linear models (GLMs).\nThe main starting resource is an earlier (incomplete) correspondence with claude which should be available here:\nhttps://claude.ai/share/4797ead7-ed2e-4e08-a432-95f307ecc39d\n(If the above is not accessible to you, let me know the best way to share it with you.\nThe other main resource is my stats website, and in particular the main course materials:\nhttps://jonminton.github.io/JonStats/\n(See Main course)\nI would be keen to see how this can be developed in a highly modular way, with extensive use of claude.md and .claude/ files to help with development across multiple sessions."
  },
  {
    "objectID": "support/claude-concept-chat.html",
    "href": "support/claude-concept-chat.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\nLetâ€™s develop a storyboard or interactive tool description for demonstrating how generalised linear models work. What are your initial ideas? Weâ€™ll â€˜riffâ€™ on this19 NovIâ€™m thinking about what makes GLMs click for people, and I reckon the key insight is seeing how the link function transforms the linear predictor into the response scale. Most people grasp linear regression intuitively, but GLMs add this extra layer of abstraction that can feel opaque. Here are a few directions we could explore: 1. The â€œfitting roomâ€ concept Show how different distributions are like different clothing that data needs to wear. Youâ€™ve got count data trying to squeeze into a normal distribution (doesnâ€™t fit!), then we try Poisson (perfect fit). The link function is the transformation that makes the linear modelâ€™s predictions compatible with each distributionâ€™s constraints. 2. Interactive link function explorer A tool where you manipulate a linear predictor (drag sliders for coefficients), and watch in real-time how different link functions (logit, log, identity, probit) transform that straight line into curves on the response scale. Split view: left side shows the linear predictor space, right side shows the transformed predictions with actual data points overlaid. 3. The â€œconstraint satisfactionâ€ narrative Frame it as: â€œWe want to do linear regression, BUT our response variable breaks the rules (negative counts? probabilities outside 0-1?). How do we make predictions that respect the constraints whilst keeping our beloved linear model?â€ Show the link function as the clever compromise. 4. Residual patterns detective Start with someone fitting regular linear regression to inappropriate data (binomial, count, gamma). Show the diagnostic plots looking terrible. Then progressively introduce: â€œWhat if we transform the response? What if we change the error distribution? What if we do both systematically?â€ Build up to the GLM framework as the principled solution. Which of these resonates? Or have they sparked a different direction entirely?Letâ€™s think how we might develop an interactive page for explaining how GLMs work.Â Think of three core components:1) Response Transformers: g(.). These transform the response as you suggest. 2) Noisemakers: f(.). These allow stochasticity to be represented. 3) Predictor transformers: h(.). These transform part of the predictor matrix X so as to make it closer to normally distributed.Could you imagine how an interactive tool could be developed which shows how different types of data can be represented using these three types of component?19 NovThis is a brilliant decomposition! Youâ€™re essentially separating the systematic component (predictors), the random component (error distribution), and the link between them. Let me sketch out how an interactive tool could work: Core Interface Structure Three-panel layout: Panel 1: Data Generator (left side)\nUser selects a â€œdata scenarioâ€ from dropdown or cards:\nâ€œHospital readmissionsâ€ (binomial) â€œInsurance claims countâ€ (Poisson/negative binomial) â€œTime to equipment failureâ€ (gamma/exponential) â€œAir quality measurementsâ€ (Gaussian)\nUnderlying truth: you generate data where X â†’ Î· (linear predictor) â†’ Î¼ (mean) â†’ Y (response)\nPanel 2: Component Workshop (centre, the meat of it) Three interactive sections stacked vertically:\nPredictor Transformers h(X)\nToggles/dropdowns for each predictor: [Raw | Log | Square root | Polynomial | Spline] Live scatter plots showing marginal distributions before/after transformation Maybe a normality test statistic updating in real-time Visual: overlaid density curves showing â€œhow normal is this now?â€\nResponse Transformers g(Î¼)\nDropdown: [Identity | Log | Logit | Probit | Inverse] Visual representation: show the function curve itself Key insight display: â€œThis maps (-âˆ, âˆ) to [a, b]â€ with the actual bounds shown\nNoisemakers f(Y|Î¼)\nDistribution family selector: [Gaussian | Poisson | Binomial | Gamma | Neg. Binomial] Interactive parameters (where relevant): dispersion, trials for binomial, etc. Visual: show probability mass/density function updating as Î¼ changes Maybe animate: â€œIf Î¼ = 5, hereâ€™s what Y values look likeâ€¦â€\nPanel 3: Diagnostic Dashboard (right side) Real-time fitting and diagnostics:\nDeviance residuals vs fitted Q-Q plot Residuals vs linear predictor A â€œbadness of fitâ€ meter (simplified deviance or AIC) Coefficient estimates with SEs\nThe Interactive Flow Mode 1: Guided Discovery\nPresent data, deliberately fit wrong model (normal identity link to count data) Diagnostics look terribleâ€”patterns everywhere Prompt user: â€œThe problem is [X]. Try changing [component Y]â€ When they fix it, diagnostics improve dramatically Positive reinforcement: â€œNotice how the residual pattern disappeared!â€\nMode 2: Sandbox Exploration\nLoad any dataset or simulation User experiments freely with all three components Tool shows â€œconventional choiceâ€ badge when they select the standard GLM for that data type But allows weird combinations: â€œPoisson with identity linkâ€”see what happens!â€\nKey Interactive Features Linked brushing:\nHover over a data point â†’ see it highlighted across all panels See its contribution to deviance, its residual, where it falls in the predictor space\nAnimation of the data generation process:\nStep through: X â†’ h(X) â†’ Î²â€™h(X) â†’ gâ»Â¹(Î²â€™h(X)) = Î¼ â†’ f(Y|Î¼) â†’ observed Y Show the transformation at each stage with actual numbers for selected observations\nâ€œWhat if?â€ sliders:\nChange Î² coefficients and watch predictions update Change dispersion parameter and see variance in f(.) widen/narrow Watch how changing h(.) affects the linearity assumption\nComparison mode:\nSplit screen: fit two different models side-by-side Direct visual comparison of residual plots and fit statistics\nTechnical Implementation Thoughts React components structure:\n - handles data generation and state  - the three transformer controls\n  \n - recharts visualisations  - probably calls a WebAssembly-compiled GLM fitter or servers-side R API\nFor actual GLM fitting: You could either:\nCall a lightweight JavaScript GLM library (glmnet.js if it exists, or roll your own for simple cases) Use WebAssembly to compile Râ€™s glm() function Server-side: hit an R/Python API endpoint (Plumber or FastAPI)\nGiven your R preference, option 3 might be cleanest for prototyping: React frontend, Plumber backend doing the heavy lifting. Educational Scaffolding Progressive disclosure:\nBeginner mode: only show appropriate transformers for each data type Intermediate: show all options but flag inappropriate ones Advanced: complete freedom + show the maths\nConceptual annotations:\nWhen user selects logit link: â€œThis is for proportions/probabilities. It maps the linear predictor (any real number) to (0,1)â€ When residual patterns appear: â€œSee this funnel shape? Thatâ€™s heteroskedasticityâ€”variance isnâ€™t constantâ€\nDoes this capture what youâ€™re imagining? The key question is whether we want this to be purely demonstrative, or also actually functional for real data analysis?Letâ€™s zoom in on the â€˜what ifâ€™ slider part for now. A key thing I want to demonstrate is that maximum likelihood is one of many theoretically possible loss functions, though one which can be strongly demonstrated from first principles. It would be good to have options to manually toggle beta and alpha (noisemaker) parameters, but also to select different algorithms and loss functions to do this too. How might that work?19 NovAh, this is excellent! You want to expose the optimisation landscape itself and show that MLE is a choice, albeit a principled one. Let me think through how this could work: Control Panel Structure Manual Controls Section:\nSliders for each Î² coefficient (Î²â‚€, Î²â‚, Î²â‚‚â€¦) Slider(s) for dispersion/scale parameters (Ï† for quasi-likelihood, overdispersion for NB, etc.) Real-time updates: as you drag, watch predictions and loss function value change\nOptimisation Section: Two dropdown menus side-by-side: 1. Loss Function Selector:\nMaximum Likelihood (deviance) Least Squares (MSE on response scale) Least Squares on transformed scale (MSE on g(Î¼) scale) Least Absolute Deviation (L1 loss) Huber loss (robust) Custom: user enters their own function?\n\nAlgorithm Selector:\n\nIteratively Reweighted Least Squares (IRLS) [the standard] Newton-Raphson Gradient Descent Nelder-Mead (derivative-free) Coordinate Descent Manual (slider control only)\nThen an â€œOptimiseâ€ button that runs the selected algorithm with the selected loss. Visual Feedback Components 1. Loss Surface Visualisation This is crucial for intuition. For 2 parameters (say Î²â‚€ and Î²â‚):\nContour plot of the loss function in (Î²â‚€, Î²â‚) space Current parameter values shown as a point As algorithm runs, show the trajectory of the optimisation path Different algorithms trace different paths to the minimum\nFor &gt;2 parameters, could show:\n2D slices through the loss surface (fix all but two parameters) Or parallel coordinates plot showing parameter evolution over iterations\n\nLoss Function Value Tracker\n\nReal-time numeric display: â€œCurrent loss: 234.56â€ Line plot showing loss decreasing over iterations when algorithm runs Compare multiple algorithms: overlaid traces showing convergence speed\n\nAlgorithm Diagnostics Panel When an algorithm is running/has run:\n\nIteration count Convergence status (gradient norm, relative change in loss) For IRLS specifically: show the working weights evolving Step size (for gradient descent variants)\n\nSplit View: Predictions Panel\n\nTop: fitted values overlaid on data Bottom: residuals Updates in real-time as Î² changes (manual or algorithmic)\nThe Interactive Workflow Workflow 1: Manual Exploration\nUser loads data (say, Poisson count data) Sets algorithm to â€œManualâ€ Moves Î² sliders around Watches:\nLoss function value change Their position move on the contour plot Fitted line move on the data plot Residual patterns shift\nTries to minimise loss themselves (gamification!) Then clicks â€œOptimise with IRLSâ€ to see the algorithm find it instantly\nWorkflow 2: Loss Function Comparison\nFit Poisson data with Maximum Likelihood â†’ get Î²Ì‚_ML Switch loss to â€œLeast Squaresâ€ â†’ optimise again â†’ get Î²Ì‚_LS Display both sets of coefficients side-by-side Show both fitted lines on the same plot Key insight: â€œSee how LS gives more weight to outliers?â€\nWorkflow 3: Algorithm Racing Split the contour plot into 3-4 panels, each running a different algorithm simultaneously from the same starting point:\nIRLS (top-left) Gradient Descent (top-right) Newton-Raphson (bottom-left) Nelder-Mead (bottom-right)\nAnimate them converging at different speeds, different paths. Some might overshoot, some take more iterations. Deep Dive: The Loss Function Maths Display When user hovers over or selects a loss function, show a formula panel: For Maximum Likelihood (Poisson): L(Î²) = Î£áµ¢ [yáµ¢ log(Î¼áµ¢) - Î¼áµ¢ - log(yáµ¢!)] where Î¼áµ¢ = exp(Xáµ¢Î²)\nDeviance: D = 2Î£áµ¢ [yáµ¢ log(yáµ¢/Î¼áµ¢) - (yáµ¢ - Î¼áµ¢)] For Least Squares: L(Î²) = Î£áµ¢ (yáµ¢ - Î¼áµ¢)Â² For Least Squares (link scale): L(Î²) = Î£áµ¢ (g(yáµ¢) - Î·áµ¢)Â² where Î·áµ¢ = Xáµ¢Î² Accompanying text: â€œMLE comes from assuming data are i.i.d. draws from f(y|Î¼). Itâ€™s not just convenientâ€”itâ€™s what probability theory tells us to maximise.â€ Demonstrating Why MLE is Special Interactive Demonstration:\nEfficiency comparison:\nGenerate data from known Î² Fit with MLE vs LS repeatedly (bootstrap/simulation) Show sampling distributions of Î²Ì‚: MLE should have tighter distribution Display: â€œMLE is asymptotically efficientâ€”smallest variance among consistent estimatorsâ€\nConsistency under misspecification:\nGenerate overdispersed Poisson (negative binomial really) Fit with Poisson MLE Show: Î²Ì‚ still consistent! (SE wrong, but estimates okay) Fit with LS: Î²Ì‚ might be biased\nThe deviance connection:\nShow that MLE naturally gives you deviance Deviance â†’ likelihood ratio tests â†’ nested model comparisons Other loss functions donâ€™t give you this inferential framework Visual: tree of nested models with deviance differences\nTechnical Implementation Details Loss function calculation (in R backend or JS): javascriptconst lossFunctions = { mle_poisson: (y, mu) =&gt; -sum(y.map((yi, i) =&gt; yi * Math.log(mu[i]) - mu[i])),\nleast_squares: (y, mu) =&gt; sum(y.map((yi, i) =&gt; Math.pow(yi - mu[i], 2))),\nleast_squares_link: (y, eta, linkFn) =&gt; sum(y.map((yi, i) =&gt; Math.pow(linkFn(yi) - eta[i], 2))),\n// etc. } Optimisation algorithms: For IRLS, youâ€™d need the working weights and adjusted dependent variable. This probably wants to live in R: r# Plumber API endpoint #* @post /optimise function(X, y, family, link, loss_fn, algorithm) { if (loss_fn == â€œmleâ€ && algorithm == â€œirlsâ€) { fit &lt;- glm(y ~ X - 1, family = family) return(list( coefficients = coef(fit), iterations = fit\\(iter,  converged = fit\\)converged, trajectory = fit$trace # if we store it )) } # â€¦ other combinations } For the contour plot:\nPre-compute loss on a grid of (Î²â‚€, Î²â‚) values Use Plotly or D3 for interactive contours Overlay the optimisation path as a line with markers at each iteration\nPedagogical Sequencing Level 1: Discovery\nHide the maths initially Just show: â€œDifferent ways to measure fitâ€ Let them see that different loss functions â†’ different Î²Ì‚\nLevel 2: Understanding\nReveal the formulas Show the connection: MLE â†”ï¸ assumed distribution Demonstrate efficiency via simulation\nLevel 3: Advanced\nShow score functions (gradient of log-likelihood) Fisher information Why IRLS works: connection to Newton-Raphson with expected rather than observed Hessian\nDoes this align with your vision? The key tension I see is between pedagogical clarity (keep it simple) and completeness (show all the options). Perhaps we need user-selectable â€œmodesâ€: Basic, Intermediate, Advanced?To avoid information overload letâ€™s think how we can guide the user one step at a time, through having one page at a time, which cannot be skipped unless interacted with. To start with the first page could just be a list of scenarios, describing types of problem that can be addressed with statistical modelling. The second page would then allow the user to see the data associated with the scenario. The third page would introduce different types of GLM, the kinds of problems they are used for, and as advanced material their likelihood function. The page after that would then describe some predictor transformers. The page after that would then be when the user starts fitting the data to the model.19 NovExcellent! A linear, guided tour structure prevents cognitive overload and builds understanding progressively. Let me sketch out this page-by-page journey:\nPage 1: Scenario Selection Title: â€œWhat are you trying to understand?â€ A card-based interface with 5-6 scenarios: Card layout (each shows an icon, title, one-sentence description):\nHospital Readmissions â€œWill patients return within 30 days? Binary outcomes from medical records.â€ Insurance Claims Volume â€œHow many claims will we receive? Count data with no upper limit.â€ Species Abundance â€œCounting organisms at different sites. Ecology data with excess zeros.â€ Equipment Lifespan â€œWhen will machines fail? Positive continuous data, right-skewed.â€ Crop Yields â€œTonnes per hectare under different treatments. Continuous with constant variance.â€ Market Survey Response â€œHow satisfied are customers? Ordinal categories from 1-5.â€\nInteraction: User clicks one card. It expands slightly, others fade. A â€œContinueâ€ button appears. Purpose: Prime them to think about data types and question types before seeing any maths.\nPage 2: Explore Your Data Title: â€œHospital Readmissions: What does the data look like?â€ Main visual: Large scatter plot or strip plot showing the data\nX-axis: a key predictor (e.g., â€œLength of initial stay (days)â€) Y-axis: binary outcome (0 = no readmission, 1 = readmission) Points jittered vertically for visibility Maybe add a smoother/loess curve overlaid\nSide panel: Data summary statistics\nN observations Proportion of 1s vs 0s Range of predictor A few example rows shown in a mini table\nInteractive elements:\nDropdown to switch which predictor is shown (if multivariate) Toggle: â€œShow raw dataâ€ vs â€œShow proportion in binsâ€ (bin the x-axis, show success rate per bin)\nKey text callout: â€œNotice: The outcome is binary (0 or 1), but when we group patients by similar characteristics, the readmission rate varies between 0% and 100%. We need a model that respects these boundaries.â€ Bottom of page: â€œContinueâ€ button (always visible once theyâ€™ve interacted with the chart) Purpose: Let them see the problem before solving it. Build intuition about what the model needs to capture.\nPage 3: Choosing Your Model Family Title: â€œDifferent data, different modelsâ€ Main content: A matrix/table layout Data TypeExampleGLM FamilyWhy this worksBinary (0/1)Readmission?Binomial (Logistic)Maps probabilities to whole real lineCountsNumber of claimsPoissonNon-negative integers onlyCounts (overdispersed)Species with many zerosNegative BinomialAllows more variance than PoissonPositive continuousTime to failureGammaPositive values, right-skewedContinuousCrop yieldGaussianSymmetric, unbounded Your scenario is highlighted (e.g., the Binomial row glows/is emphasised) Interactive element:\nClick on each row to expand it Expansion shows:\nA small plot of the probability/density function shape The link function (name only at this stage): â€œlogitâ€, â€œlogâ€, â€œidentityâ€ One-sentence: â€œUse this whenâ€¦â€\nâ€œShow me the mathsâ€ toggle (collapsed by default): When expanded, shows for the highlighted row: For Binomial/Logistic: Pr(Y = 1 | X) = Ï€ Ï€ = gâ»Â¹(XÎ²), where g(Ï€) = log(Ï€/(1-Ï€)) [logit link]\nLikelihood: L(Î²) = âˆáµ¢ Ï€áµ¢Ê¸â±(1-Ï€áµ¢)â½Â¹â»Ê¸â±â¾ Log-likelihood: â„“(Î²) = Î£áµ¢ [yáµ¢ log(Ï€áµ¢) + (1-yáµ¢) log(1-Ï€áµ¢)] Key pedagogical text: â€œThe GLM family determines two things: (1) what values Y can take, and (2) how the variance relates to the mean. Maximum likelihood uses this information to find the best-fitting Î².â€ Bottom: â€œContinue with Binomial GLMâ€ button Purpose: Connect data type â†’ model family. Plant the seed that likelihood depends on distributional assumptions.\nPage 4: Preparing Your Predictors Title: â€œShould we transform the predictors?â€ Context reminder (small banner at top): â€œYouâ€™re modelling hospital readmissions (binary) using length of stay as a predictor.â€ Main visual: Split panel Left panel: Current predictor\nHistogram of X (length of stay) Summary stats: mean, SD, range Shape description: â€œRight-skewed, most stays are short, a few very longâ€\nRight panel: Transformation preview\nDropdown menu: [None | Log | Square root | Polynomial (degree 2) | Polynomial (degree 3)] When selected, shows histogram of transformed X Summary stats update Shape description updates: â€œLog transform makes distribution more symmetricâ€\nInteractive scatter plot below: Shows Y vs X, with transformation applied to X-axis in real-time\nOriginal scale markers remain visible (secondary axis) Overlay a smoothed trend line Text updates: â€œSee how log-transform makes the relationship more linear?â€\nGuidance text: â€œGLMs assume the relationship between predictors and the link-transformed outcome is linear. Transforming predictors can help achieve this. However, interpretation becomes harder.â€ â€œWhy transform?â€ expandable section:\nLinearise relationships Handle skewness Make effects multiplicative (log) vs additive (raw) Reduce influence of outliers\nSelection mechanism: Radio buttons:\nâ—‹ Use raw predictor (easier to interpret) â—‹ Use log(predictor) (more linear relationship) â—‹ Let me try othersâ€¦ [reveals more options]\nBottom: â€œContinue with [selected transformation]â€ button Purpose: Introduce predictor transformations separately from link functions (which come next). Show the visual impact immediately.\nPage 5: Fitting Your First Model Title: â€œFinding the best-fitting lineâ€ Context banner: â€œModel: Binomial GLM with logit link. Predictor: log(length of stay)â€ Main interaction area: Top half: The Model\nLarge equation display:\nlog(Ï€/(1-Ï€)) = Î²â‚€ + Î²â‚ Ã— log(length of stay)\nManual sliders for Î²â‚€ and Î²â‚ Current values displayed: Î²â‚€ = [value], Î²â‚ = [value] Live loss display: â€œLog-likelihood: -234.56â€\nBottom half: The Visualisation Two side-by-side plots:\nData space (left):\nPoints (jittered binary outcomes) Fitted probability curve (updating live as sliders move) Y-axis: Probability of readmission (0 to 1) X-axis: Log(length of stay)\nLink space (right):\nSame X-axis Y-axis: log(Ï€/(1-Ï€)) (the linear predictor) Fitted line (straight! because weâ€™re in link space) Observed logit(empirical proportions) as points\nGamification element: â€œTry to maximise the log-likelihood by moving the sliders! Target: -198.34â€\nAs user moves sliders, if they get close: â€œYouâ€™re getting warmer!â€ Personal best tracker\nThen: Algorithmic fitting section (appears after theyâ€™ve played with sliders): Button panel:\nâ€œFit with IRLS (standard GLM)â€ â€œFit with gradient descentâ€ â€œFit with Nelder-Meadâ€\nWhen clicked:\nAnimation shows Î² values changing (sliders move automatically) Loss value decreases Iteration counter Small text: â€œConverged in X iterationsâ€\nResults panel (appears after fitting): Coefficients: Î²â‚€ = -1.23 (SE: 0.15) Î²â‚ = 0.67 (SE: 0.08)\nInterpretation: Each unit increase in log(stay) multiplies the odds of readmission by exp(0.67) = 1.95 Bottom navigation:\nâ€œTry a different loss function â†’â€ (leads to Page 6) â€œSee diagnostics â†’â€ (alternative path)\nPurpose: The culmination! Theyâ€™ve seen the data, chosen the model, prepared predictors, and now they fit. The manual sliders build intuition before algorithmic magic happens.\nPage 6 (Optional Advanced): Comparing Loss Functions Title: â€œIs maximum likelihood special?â€ Split screen showing three identical model setups side-by-side:\nMaximum Likelihood (deviance) Least Squares (on probability scale) Least Absolute Deviation\nEach shows:\nThe fitted curve The coefficient estimates A â€œbadness of fitâ€ metric\nInteractive:\nFit all three simultaneously (animated) Compare the resulting curves visually Table comparing Î²Ì‚ estimates Residual plots for each\nKey insight callout: â€œMLE gives you the most efficient estimates when your distributional assumption (Binomial) is correct. Other loss functions might be more robust but sacrifice efficiency.â€\nTechnical Navigation Structure Progress bar at top of every page showing: Scenario â†’ Data â†’ Model â†’ Predictors â†’ Fitting â†’ [Advanced] Lock mechanism: Canâ€™t proceed until minimum interaction on each page:\nPage 1: Must select scenario Page 2: Must interact with plot (hover, toggle view) Page 3: Must read about their model family (tracked by scroll or expansion) Page 4: Must try at least one transformation Page 5: Must either manually adjust sliders OR run an algorithm\nReact router structure: /scenario /data/:scenarioId /model/:scenarioId /predictors/:scenarioId/:modelFamily /fit/:scenarioId/:modelFamily/:transformation /compare/:scenarioId/:modelFamily/:transformation (optional) State passed between pages via context or URL parameters.\nDoes this flow feel right? The key pedagogical moves are:\nConcrete before abstract (see data before equations) One concept per page (donâ€™t mix link functions and loss functions) Active before passive (manipulate before optimise) Build intuition before automation (manual sliders before algorithms)\nWhat would you modify or emphasise differently?Page 2: I think just a table of the data, paginated, would be sufficient as we may have different variables.Remember potentially transforms of response variables, such as log transforms and/or adding continuity corrections.Also double check about best model for excess zeros. Wouldnâ€™t this potentially require zero inflation not just negative binomial?19 NovYouâ€™re absolutely right on both counts. Page 2 Revision Simple paginated table makes much more sense:\nShows first 20 rows by default Columns: all variables in the dataset Basic pagination controls (Next/Previous, jump to page) Column headers with data type indicators (numeric, binary, count, etc.) Summary row at bottom: N, means/proportions, ranges\nMaybe just one summary visualisation option: a dropdown saying â€œQuick look atâ€¦â€ that lets them pick one variable to see a histogram/bar chart of. But the table is the main event. This is cleaner and scales to multivariate scenarios without cramming everything into one plot.\nExcess Zeros - Youâ€™re Right I was sloppy. Negative binomial handles overdispersion (variance &gt; mean) but doesnâ€™t specifically model excess zeros. For that you need: Zero-Inflated Models:\nZero-Inflated Poisson (ZIP) Zero-Inflated Negative Binomial (ZINB)\nOr alternatively: Hurdle Models:\nTwo-stage: (1) Binary model for zero vs non-zero, (2) Truncated count model for positive values\nSo the Page 3 table should be more nuanced: Data TypeIssueModel FamilyWhyCountsOverdispersionNegative BinomialVariance &gt; mean, but zeros still from count processCountsExcess zerosZero-Inflated Poisson/NBTwo processes: structural zeros + count processCountsExcess zeros (alternative)Hurdle ModelSeparate models for zero vs positive For the species abundance scenario, the description should flag: â€œMany sites have zero organismsâ€”more than a Poisson would predict. This suggests two processes: presence/absence, then abundance given presence.â€\nResponse Transformers Good catchâ€”Iâ€™d buried these in the noisemaker component, but they deserve explicit treatment. Let me think about where they fit in the page sequence: Option A: Add them to Page 4 (Predictor Transformers) Rename to â€œPage 4: Transformersâ€ and split into two sections:\nPredictor transformations: h(X) Response transformations: h(Y) [for Gaussian models mainly]\nShow that for Gaussian models, transforming Y (e.g., log(Y)) is one approach, but GLMs with a log link are another approach that achieves similar goals whilst keeping Y on its natural scale. Option B: Address them in Page 3 (Model Selection) When discussing Gamma models or Gaussian models, note:\nâ€œFor positive, right-skewed continuous data, you could: (1) log-transform Y and use Gaussian GLM with identity link, or (2) keep Y untransformed and use Gamma GLM with log link. The latter is often preferable as it handles zeros better (with continuity correction if needed) and respects the variance structure.â€\nI think Option B is cleaner pedagogicallyâ€”it positions response transformations as the â€œold wayâ€ that GLMs improve upon, rather than presenting them as separate equal tools. Continuity corrections: These become relevant for:\nLog-transforming responses with zeros: log(Y + c) where c is small (e.g., 0.5, 1) Logit-transforming proportions that are exactly 0 or 1: (Y + c)/(n + 2c)\nShould probably mention these as a practical note when discussing log/logit links:\nâ€œIf you have zeros in your response, a log link requires adding a small constant firstâ€”but this is arbitrary. Better to use a distribution that naturally handles zeros (Poisson, ZIP) or model the zeros separately (hurdle).â€\nDoes this clean up those issues? The zero-inflation point is particularly importantâ€”I should have been more careful distinguishing overdispersion from excess zeros.A key interactive process Iâ€™d like to see would look as follows:After selecting the dataset.First get user to select the response column, y. On selecting this it gets pulled to the right hand side of the screenThen ask user to select one or more remaining columns to form the predictor matrix X. These get moved to the left hand side of the screen. There will now be a line going into y, from the leftAnd lines going out of each variable of X, going right.Then once y and X are selected, the next step shown to the user will be to have a switch. These are the transforms h() on predictors. And then a switch for the line going right into y. The transform on response.Then the user gets to select transformer g() and noisemaker f()Show more19 NovOh, this is brilliant! A visual flow diagram that builds up the model specification piece by piece. This makes the entire GLM structure transparent and interactive. Let me sketch this out:\nThe Interactive Model Builder Interface Stage 1: Select Response Variable Visual layout:\nCentre of screen: the full data table (paginated) Instruction banner: â€œWhich variable are you trying to predict or explain?â€\nInteraction:\nUser clicks a column header That column slides/animates to the right side of screen Appears in a box labeled Y (Response) Shows: variable name, data type, sample values Quick diagnostic: range, mean/proportion, distribution shape thumbnail\nStage 2: Select Predictor Variables Instruction updates: â€œWhich variables might help explain [response variable name]?â€ Interaction:\nUser clicks one or more remaining column headers Each selected column slides/animates to the left side of screen Stack vertically in boxes labeled Xâ‚, Xâ‚‚, Xâ‚ƒâ€¦ Each shows: variable name, data type, sample values\nVisual connection appears:\nAnimated lines/arrows from each X box flowing rightward Currently these just point towards the centre (towards Y) The lines are initially thin/grey, labelled â€œrawâ€\nStage 3: Transform Predictors Instruction: â€œShould we transform any predictors before using them?â€ Visual changes:\nEach X box now has a dropdown/switch control appear on it Options: [Raw | Log | âˆš | xÂ² | xÂ³ | Spline] When user selects a transform:\nThe box updates: â€œXâ‚â€ â†’ â€œlog(Xâ‚)â€ The line flowing from it changes colour/thickness A small icon appears on the line showing the transform: hâ‚() Preview panel shows before/after histogram\nAdvanced toggle: â€œWhy transform predictors?â€\nExpandable text about linearity assumptions Mini scatter plot: Y vs X, then Y vs h(X) showing improved linearity\nStage 4: Combine Predictors (Linear Predictor) Instruction: â€œThese predictors combine to form a linear predictor: Î· = Î²â‚€ + Î²â‚hâ‚(Xâ‚) + Î²â‚‚hâ‚‚(Xâ‚‚) + â€¦â€ Visual changes:\nThe lines from all X boxes converge into a central node This node is labeled Î· (linear predictor) Itâ€™s positioned centre-left of the screen The equation appears inside or below the node Lines are labeled with Î²â‚, Î²â‚‚, etc.\nOptional interaction here:\nClick on the Î· node to see the equation expanded Preview: â€œThis can produce any value from -âˆ to +âˆâ€\nStage 5: Link Function (Response Transformer) Instruction: â€œHow should we transform the linear predictor to get predictions for Y?â€ Visual changes:\nA new line flows rightward from Î· towards Y A transform node appears on this line, labeled gâ»Â¹() This node has a dropdown:\nFor binary Y: [Logit | Probit | Complementary log-log] For count Y: [Log | Identity | âˆš] For continuous Y: [Identity | Log | Inverse]\nWhen user selects:\nThe node updates: â€œgâ»Â¹(Î·) = Î¼ (expected value)â€ The line changes colour A function visualisation appears:\nSmall graph showing gâ»Â¹(Î·): x-axis is Î· (-âˆ to +âˆ), y-axis is Î¼ (constrained range) e.g., logit shows sigmoid curve from 0 to 1 e.g., log shows exponential curve from 0 to +âˆ\nKey text callout: â€œThe link function ensures predictions respect constraints: probabilities stay between 0 and 1, counts stay positive, etc.â€ Stage 6: Noisemaker (Distribution Family) Instruction: â€œReal data has noise. How are observations scattered around their expected values?â€ Visual changes:\nThe line arriving at Y now fans out into multiple possible values A distribution selector appears near the Y box Options depend on Y type:\nBinary: [Binomial] (only option, greyed out with explanation) Count: [Poisson | Negative Binomial | Zero-Inflated Poisson] Continuous: [Gaussian | Gamma | Inverse Gaussian]\nWhen user selects:\nThe Y box updates to show the distribution: â€œY ~ Poisson(Î¼)â€ A probability function visualisation appears:\nFor fixed Î¼ value(s), show the PMF/PDF Animate: â€œIf Î¼ = 5, here are possible Y valuesâ€¦â€ Slider to change Î¼ and watch distribution shift/reshape\nAdditional parameters:\nSome distributions need extras:\nNegative Binomial: dispersion parameter Î¸ Gaussian: variance ÏƒÂ²\nThese appear as additional controls/sliders\nKey text callout: â€œThis distribution defines the likelihood function that weâ€™ll maximise to find the best Î² values.â€\nComplete Visual Summary At the end, the full flow is visible on screen: [Xâ‚] â”€hâ‚()â†’ â” [Xâ‚‚] â”€hâ‚‚()â†’ â”œâ”€â†’ [Î· = XÎ²] â”€gâ»Â¹()â†’ [Î¼] ~f()~&gt; [Y] [Xâ‚ƒ] â”€hâ‚ƒ()â†’ â”˜ â†“ (observed data) Legend:\nh() = predictor transformers (log, sqrt, polynomial, etc.) Î· = linear predictor (XÎ²) gâ»Â¹() = inverse link function (logitâ»Â¹, exp, identity) Î¼ = expected value of Y f() = probability distribution (Binomial, Poisson, Gaussian, etc.) Y = observed response data\nInteractive Polish Hover behaviours:\nHover over any transform node â†’ see formula and brief explanation Hover over connections â†’ highlight the full path Hover over Y â†’ see summary stats and distribution shape\nCollapsible detail levels:\nBeginner: just show icon labels on transforms Intermediate: show mathematical notation Advanced: show full equations, variance functions, canonical links\nValidation:\nIf user selects incompatible combinations, gentle warning appears\ne.g., â€œIdentity link with Poisson allows negative predictionsâ€”are you sure?â€ Or just grey out illogical combinations\nâ€œShow me a valid modelâ€ hint button:\nIf user seems stuck, suggest a conventional choice â€œFor count data, try: Log link + Poisson distributionâ€\nNavigation Flow After completing this model specification: Bottom of screen navigation:\nâ€œâ† Back to dataâ€ (can restart) â€œContinue to fitting â†’â€ (proceeds to your slider/optimization page)\nState preservation:\nAll choices encoded in URL or context Can bookmark/share a specific model configuration\nTechnical Implementation Notes React component structure: jsx  // initially full screen      Animation library:\nFramer Motion for smooth transitions React Flow or similar for the node-edge diagram D3 for the small function visualisation graphs\nThe flow diagram itself: Could use React Flow (https://reactflow.dev/):\nNodes: X variables, Î·, Î¼, Y Edges: the transformation connections Custom nodes with dropdowns embedded Drag-and-drop could even allow reordering\nDoes this capture what youâ€™re imagining? The key pedagogical win is making the entire model specification visible as a data flow graphâ€”nothing is hidden in dropdown menus. Every component of the GLM is a visible, interactive element. What would you add or change to this interaction flow?Yes. Please ensure y (observed response) and Y (modelled response) are distinguished. Y is where you placed it. y is a label on observed data which you correctly place below it.This will now help set up for the next page, where we show how the model has to be â€˜tunedâ€™, with g() tuned by Beta and f() tuned by alpha (which is sigma in most cases)19 NovExcellent distinction! Let me refine that:\nStage 6 Revised: Noisemaker Visual The complete right-hand side now shows: [Î· = XÎ²] â”€gâ»Â¹()â†’ [Î¼] â”€f(Â·|Î¼,Î±)â†’ [Y ~ f(Î¼, Î±)] â†“ [y (observed)] Key labels:\nÎ¼ = Expected value (systematic component output) Y = Random variable following distribution f y = The actual data we observed (shown as points/table below)\nVisual representation:\nY box: Shows the distribution family name and random variable: â€œY ~ Poisson(Î¼)â€ y box (below): Shows actual data table/visualization, labeled â€œy (observed data)â€ A dotted/dashed line connects Y to y: â€œone realization from this distributionâ€\nInteractive element:\nAnimate multiple draws: â€œIf we could re-run this process, weâ€™d get different y values each timeâ€ Show 3-4 simulated datasets briefly appearing in the y box, then return to actual observed data Text: â€œOur observed y is one possible outcome from Yâ€\nTransition to Next Page: â€œTuning the Modelâ€ End of model specification page: Summary panel appears: Your model specification: â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\nPredictors (X): â€¢ log(length_of_stay) â€¢ age\nâ€¢ prior_admissions\nLinear predictor: Î· = Î²â‚€ + Î²â‚Â·log(length_of_stay) + Î²â‚‚Â·age + Î²â‚ƒÂ·prior_admissions\nLink function: Î¼ = gâ»Â¹(Î·) = logitâ»Â¹(Î·) = exp(Î·)/(1 + exp(Î·))\nDistribution: Y ~ Binomial(n=1, Ï€=Î¼) [i.e., Bernoulli]\nObserved data: y (1024 observations) Key insight box: â€œNow we need to find the best values for:\nÎ² = (Î²â‚€, Î²â‚, Î²â‚‚, Î²â‚ƒ) â€” these â€˜tuneâ€™ the systematic part gâ»Â¹(XÎ²) In this case, Î± is fixed (Binomial has no extra dispersion parameter)\nDifferent data y would suggest different optimal Î² values. Maximum likelihood finds the Î² that makes our observed y most probable.â€ Button: â€œContinue to fitting â†’â€\nPage: Fitting and Tuning Title: â€œFinding Î²: Tuning the model to match your dataâ€ Page layout: Top Section: Parameter Controls Left side: Î² parameters (systematic component) Coefficients (Î²): Î²â‚€ (Intercept): [slider: -5 to 5] currently: 0.0 Î²â‚ (log_stay): [slider: -3 to 3] currently: 0.0\nÎ²â‚‚ (age): [slider: -0.5 to 0.5] currently: 0.0 Î²â‚ƒ (prior_admissions): [slider: -2 to 2] currently: 0.0 Right side: Î± parameters (random component) For this Binomial case: Distribution parameters (Î±): (None for Bernoulli â€” Ï€ is fully determined by Î¼) But for other examples: For Gaussian: ÏƒÂ² (variance) [slider: 0.1 to 10] For Neg. Binomial: Î¸ (dispersion) [slider: 0.1 to 20]\nFor Gamma: shape parameter [slider: 0.5 to 10] Key distinction highlighted:\nÎ² controls where the expected values Î¼ are (via gâ»Â¹(XÎ²)) Î± controls how spread out Y is around Î¼ (the noise level)\nMiddle Section: Live Visualizations Three-panel view: Panel 1: Linear predictor space Î· = XÎ² (for each observation i) Histogram of Î·áµ¢ values Shows: as Î² changes, this distribution shifts Panel 2: Response space Fitted values Î¼áµ¢ = gâ»Â¹(Î·áµ¢) Scatter: observed y vs.Â fitted Î¼ For binary: jittered points with fitted probability curve As Î² changes, the curve reshapes Panel 3: Likelihood landscape Log-likelihood: â„“(Î²|y) = Î£áµ¢ log f(yáµ¢ | Î¼áµ¢, Î±)\nReal-time value: â„“ = -234.56\nFor 2 parameters: contour plot in (Î²â‚, Î²â‚‚) space Current position marked with crosshairs Target: the peak! Bottom Section: Loss Function Display Formula shown: For Binomial: â„“(Î²) = Î£áµ¢ [yáµ¢ log(Î¼áµ¢) + (1-yáµ¢) log(1-Î¼áµ¢)]\nwhere Î¼áµ¢ = logitâ»Â¹(Î²â‚€ + Î²â‚xâ‚áµ¢ + Î²â‚‚xâ‚‚áµ¢ + Î²â‚ƒxâ‚ƒáµ¢) Expandable â€œWhy maximize this?â€: â€œThis likelihood measures: how probable is our observed data y, given parameters Î² and Î±? Maximum likelihood estimation (MLE) finds the Î² that makes y most probable â€” the best explanation for what we observed.â€\nThe Tuning Process Manual tuning phase:\nUser moves Î² sliders All three panels update in real-time:\nÎ· distribution shifts Fitted Î¼ values change, curve reshapes Likelihood value changes Position moves on contour plot\nGamification: â€œTry to maximize â„“! Target: -189.23â€\nThen: Algorithmic tuning Buttons appear:\nâ€œOptimize Î² with IRLS (standard)â€ â€œOptimize Î² with gradient descentâ€ â€œOptimize (Î², Î±) jointlyâ€ [if Î± exists and isnâ€™t fixed]\nWhen clicked:\nSliders animate to optimal values Iteration counter shows progress Path traced on contour plot Convergence message\nResults panel: Converged in 6 iterations!\nOptimal parameters: Î²Ì‚â‚€ = -2.13 (SE: 0.31) Î²Ì‚â‚ = 0.67 (SE: 0.09) Î²Ì‚â‚‚ = 0.03 (SE: 0.01) Î²Ì‚â‚ƒ = 0.45 (SE: 0.12)\nMaximum log-likelihood: -189.23\nInterpretation: - Each additional day of log(stay) multiplies odds of readmission by exp(0.67) = 1.95 - Each additional year of age multiplies odds by exp(0.03) = 1.03 - Each prior admission multiplies odds by exp(0.45) = 1.57\nSpecial Attention: When Î± Exists For models with dispersion parameters (Gaussian, Negative Binomial, Gamma), show: Two-stage optimization:\nâ€œProfile likelihoodâ€ approach:\nFix Î±, optimize Î² Then fix Î², optimize Î± Iterate until convergence Show this as an animated cycle\nJoint optimization:\nOptimize (Î², Î±) simultaneously Show 3D likelihood surface (if 1 Î² + Î±, i.e., 2D)\nVisual distinction:\nÎ² sliders are blue (systematic) Î± sliders are red (random/noise) Likelihood depends on both: â„“(Î², Î± | y)\nPage Flow Summary User journey:\nSpecify model structure (previous page): h(), g(), f() Understand what needs tuning: Î² for systematic part, Î± for random part Manually explore parameter space Let algorithms find optimum Interpret results\nNext page options:\nâ€œCompare different loss functions â†’â€ (your Page 6 idea) â€œCheck model diagnostics â†’â€ (residuals, goodness-of-fit) â€œTry a different model specification â†â€ (back to revise g() or f())\nDoes this properly distinguish y vs Y and set up the Î² vs Î± tuning distinction clearly? The visual flow from Î¼ â†’ Y â†’ y is crucial for understanding where randomness enters the model.Yes. Please ensure y (observed response) and Y (modelled response) are distinguished. Y is where you placed it. y is a label on observed data which you correctly place below it.This will now help set up for the next page, where we show how the model has to be â€˜tunedâ€™, with g() tuned by Beta and f() tuned by alpha (which is sigma in most cases)19 NovPerfect! Let me make sure I have the notation crystal clear:\nPrecise Notation Structure The flow diagram shows: [Xâ‚] â”€hâ‚()â†’ â” [Xâ‚‚] â”€hâ‚‚()â†’ â”œâ”€â†’ [Î· = XÎ²] â”€gâ»Â¹(Â·;Î²)â†’ [Î¼] â”€f(Â·;Î¼,Î±)â†’ [Y] [Xâ‚ƒ] â”€hâ‚ƒ()â†’ â”˜ â†“ [y] (observed) Key distinctions:\ngâ»Â¹(Â·; Î²): The inverse link function, parameterized by Î²\nReally itâ€™s gâ»Â¹(XÎ²), so Î² â€œtunesâ€ which values of Î¼ we get The link function itself isnâ€™t parameterized, but the linear predictor going into it is\nf(Â·; Î¼, Î±): The probability distribution, parameterized by both Î¼ and Î±\nÎ¼ comes from gâ»Â¹(XÎ²) â€” so Î² indirectly tunes this Î± is the dispersion/scale parameter (Ïƒ for Gaussian, Î¸ for Negative Binomial, etc.)\nY: The random variable â€” what the model says could happen\nY ~ f(Î¼, Î±) Each observation Yáµ¢ ~ f(Î¼áµ¢, Î±)\ny: The actual observed data â€” what did happen\nSpecific realization: y = (yâ‚, yâ‚‚, â€¦, yâ‚™) Shown below Y with arrow indicating â€œone realizationâ€\nTransition Text to Tuning Page At end of model specification: â€œYour model is now fully specified: Structure defined:\nPredictor transformations h(X) Link function gâ»Â¹(Â·) Distribution family f(Â·)\nParameters to tune:\nÎ²: tunes gâ»Â¹(Â·) â€” determines where Î¼ lands based on X Î±: tunes f(Â·) â€” determines how spread out Y is around Î¼\nWe need to find the Î² and Î± values that best explain our observed data y.â€\nNext Page: â€œTuning Î² and Î±â€ Title: â€œFinding the parameters that fit your dataâ€ Opening explanation panel: Your model has two types of parameters:\nâ”Œâ”€ Î² (systematic parameters) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Controls: WHERE the expected values are â”‚ â”‚ Via: Î¼ = gâ»Â¹(XÎ²) â”‚ â”‚ In your model: Î² = (Î²â‚€, Î²â‚, Î²â‚‚, â€¦) â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\nâ”Œâ”€ Î± (dispersion parameter) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Controls: HOW SPREAD OUT observations â”‚ â”‚ are around their expected valuesâ”‚ â”‚ Via: Var(Y) = V(Î¼) Ã— Î± â”‚ â”‚ In your model: Î± = ÏƒÂ² [for Gaussian] â”‚ â”‚ Î± = Î¸ [for Neg. Binomial] â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\nGoal: Find (Î²Ì‚, Î±Ì‚) that maximize: â„“(Î², Î±) = Î£áµ¢ log f(yáµ¢; Î¼áµ¢(Î²), Î±)\nInteractive Parameter Controls Split layout: Left Panel: Î² Parameters (Systematic) Linear predictor: Î· = XÎ² Controls the expected values Î¼ = gâ»Â¹(Î·)\nÎ²â‚€ (Intercept): [â”â”â”â—â”â”â”â”â”â”] 0.00 [-5 â†â†’ 5]\nÎ²â‚ (log_stay): [â”â”â”â—â”â”â”â”â”â”] 0.00\n[-3 â†â†’ 3]\nÎ²â‚‚ (age): [â”â”â”â—â”â”â”â”â”â”] 0.00 [-0.5 â†â†’ 0.5]\nCurrently: Î¼Ì„ = 0.47 (mean fitted probability) Right Panel: Î± Parameter (Random) For Gaussian: Noise level: Var(Y|Î¼) = ÏƒÂ²\nÏƒÂ² (variance): [â”â”â”â—â”â”â”â”â”â”] 1.00 [0.1 â†â†’ 10]\nCurrently: typical deviation = Â±1.0 For Negative Binomial: Overdispersion: Var(Y|Î¼) = Î¼ + Î¼Â²/Î¸\nÎ¸ (dispersion): [â”â”â”â—â”â”â”â”â”â”] 1.00 [0.1 â†â†’ 20]\nCurrently: variance is 2Ã— mean (Î¸=1 â†’ moderate overdispersion) Lower Î¸ â†’ more overdispersion For Binomial (Bernoulli): No extra parameter needed! Var(Y|Î¼) = Î¼(1-Î¼) [variance determined by mean]\nÎ± is fixed by the distribution\nVisualization Panels (updating in real-time) Panel 1: How Î² affects Î¼ Left plot: Linear predictor Î· = XÎ² â€¢ Histogram of Î·áµ¢ for all observations â€¢ As Î² changes, this distribution shifts/spreads\nRight plot: Expected values Î¼ = gâ»Â¹(Î·)\nâ€¢ Histogram of Î¼áµ¢ for all observations â€¢ Shows: Î² controls where Î¼ values concentrate Panel 2: How Î± affects the spread of Y around Î¼ For a few example observations, show:\nÎ¼â‚ = 0.3 â†’ Yâ‚ ~ f(0.3, Î±) [probability distribution shown] Î¼â‚‚ = 0.7 â†’ Yâ‚‚ ~ f(0.7, Î±) [probability distribution shown]\nAs Î± changes: - Gaussian: curves widen/narrow (Ïƒ changes) - Neg. Binomial: variance increases/decreases relative to mean - Gamma: shape of distribution changes\nOverlay observed y values to show fit Panel 3: Data vs.Â Model Scatter plot: observed y vs.Â fitted Î¼\n\nPerfect fit would have all points on y = Î¼ line\nÎ² determines where the fitted values fall\nÎ± determines how much scatter around the line we expect\n\nVisual: - Points are observed (yáµ¢, Î¼áµ¢) pairs - Shaded envelope shows Â±1 SD based on Î± - As Î± increases, envelope widens\nLikelihood Display Real-time updating: Log-likelihood: â„“(Î², Î± | y) = Î£áµ¢ log f(yáµ¢; Î¼áµ¢(Î²), Î±)\nCurrent value: -234.56\nThis measures: How probable is our observed data y, given parameters Î² and Î±?\nGoal: Maximize this! (find the Î² and Î± that make y most likely) For models with Î±: Note: Some algorithms fix Î± first, optimize Î², then update Î±. This is called â€œREMLâ€ or â€œprofile likelihoodâ€ when appropriate.\nOptimization Controls Manual exploration first:\nUser adjusts sliders All visualizations update Likelihood value changes â€œTry to maximize â„“!â€\nThen algorithmic: Optimize Î² (with Î± fixed): [Button: IRLS] [Button: Newton-Raphson] [Button: Gradient Descent]\nOptimize Î± (with Î² fixed):\n[Button: Profile Likelihood] [Button: REML]\nOptimize (Î², Î±) jointly: [Button: Joint Optimization] Animation shows:\nParameters converging Likelihood increasing Iteration count Convergence diagnostics\nResults Display Optimization complete!\nSystematic parameters (Î²): Î²Ì‚â‚€ = -2.13 (SE: 0.31) Î²Ì‚â‚ = 0.67 (SE: 0.09)\nÎ²Ì‚â‚‚ = 0.03 (SE: 0.01)\nDispersion parameter (Î±): ÏƒÌ‚Â² = 1.45 (SE: 0.08) [for Gaussian] or Î¸Ì‚ = 2.34 (SE: 0.42) [for Negative Binomial]\nMaximum log-likelihood: -189.23\nAIC: 386.46 (penalizes model complexity) BIC: 398.21 (penalizes complexity more heavily)\nDoes this correctly distinguish:\nÎ² tunes g() via the linear predictor XÎ² that feeds into gâ»Â¹ Î± tunes f() as the dispersion/scale parameter Y is the random variable (model) y is the observed data (reality)\nAnd sets up clearly that we need to find optimal (Î²Ì‚, Î±Ì‚) to fit the model to data?"
  },
  {
    "objectID": "test.html",
    "href": "test.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "pages/index.html",
    "href": "pages/index.html",
    "title": "Understanding GLMs",
    "section": "",
    "text": "Different types of data require different statistical models. Select a scenario below to explore how Generalised Linear Models (GLMs) can help.\n\n\n\n\nExample: Predicting maximum heart rate from patient characteristics\nResponse: Continuous numeric values (e.g., 100-200 bpm)\nGLM family: Gaussian with identity link\nStart Tutorial â†’ Free Explore â†’\n\n\n\n\n\nExample: Predicting heart disease presence/absence\nResponse: Yes/No, 0/1\nGLM family: Binomial with logit link\nComing soon\n\n\n\n\n\nExample: Number of insurance claims per customer\nResponse: 0, 1, 2, 3, â€¦\nGLM family: Poisson with log link\nComing soon"
  },
  {
    "objectID": "pages/index.html#what-problem-are-you-trying-to-solve",
    "href": "pages/index.html#what-problem-are-you-trying-to-solve",
    "title": "Understanding GLMs",
    "section": "",
    "text": "Different types of data require different statistical models. Select a scenario below to explore how Generalised Linear Models (GLMs) can help.\n\n\n\n\nExample: Predicting maximum heart rate from patient characteristics\nResponse: Continuous numeric values (e.g., 100-200 bpm)\nGLM family: Gaussian with identity link\nStart Tutorial â†’ Free Explore â†’\n\n\n\n\n\nExample: Predicting heart disease presence/absence\nResponse: Yes/No, 0/1\nGLM family: Binomial with logit link\nComing soon\n\n\n\n\n\nExample: Number of insurance claims per customer\nResponse: 0, 1, 2, 3, â€¦\nGLM family: Poisson with log link\nComing soon"
  },
  {
    "objectID": "pages/index.html#how-this-guide-works",
    "href": "pages/index.html#how-this-guide-works",
    "title": "Understanding GLMs",
    "section": "How this guide works",
    "text": "How this guide works\nThis interactive guide will walk you through:\n\nSelecting your variables - Choose which variable to predict (response) and which variables to use as predictors\nBuilding the model - Understand the three components: predictor transformers h(X), link function g(), and distribution family f()\nFitting parameters - See how maximum likelihood estimation finds the best Î² coefficients\nInterpreting results - Understand what your model tells you about the data\n\nEach step builds on the previous one, revealing the GLM framework piece by piece."
  },
  {
    "objectID": "pages/index.html#the-glm-framework",
    "href": "pages/index.html#the-glm-framework",
    "title": "Understanding GLMs",
    "section": "The GLM Framework",
    "text": "The GLM Framework\nAt its core, a GLM connects predictors to a response through this structure:\nPredictors â†’ Linear combination â†’ Link function â†’ Expected value â†’ Distribution â†’ Observed data\n    X            Î· = XÎ²              gâ»Â¹(Î·)            Î¼              f(Î¼,Î±)           y\n\nÎ² (beta): Coefficients that weight each predictor\ng(): Link function that maps between linear predictor and expected value\nf(): Distribution family that describes how observations scatter around their expected values\nY: The random variable (what the model predicts could happen)\ny: The observed data (what actually happened)\n\nReady to explore?\n\nStart the guided tutorial â†’ (recommended for beginners)\nFree explore mode â†’ (if you already know the basics)"
  },
  {
    "objectID": "pages/tutorial/model-builder.html",
    "href": "pages/tutorial/model-builder.html",
    "title": "Tutorial: Building Your First Model",
    "section": "",
    "text": "Code\ndatasetMeta = FileAttachment(\"../../data/heart.json\").json()\n\n// Tutorial script definition\ntutorialScript = {\n  return {\n    title: \"Predicting Maximum Heart Rate\",\n    description: \"Learn to build a GLM by predicting maximum heart rate from patient characteristics\",\n    steps: [\n      {\n        id: 1,\n        instruction: \"First, let's select what we want to predict. Drag **Max Heart Rate** to the Response zone on the right.\",\n        target: { variable: \"thalach\", zone: \"response\" },\n        hint: \"The response variable (y) is the outcome we're trying to explain or predict.\",\n        why: \"Maximum heart rate during exercise is a key indicator of cardiovascular fitness.\",\n        highlightVariables: [\"thalach\"]\n      },\n      {\n        id: 2,\n        instruction: \"Now add your first predictor. **Age** is likely to affect maximum heart rate. Drag it to the Predictors zone.\",\n        target: { variable: \"age\", zone: \"predictor\" },\n        hint: \"Predictors (X) are variables we think might explain variation in the response.\",\n        why: \"Older patients typically have lower maximum heart rates - this is well established in cardiology.\",\n        highlightVariables: [\"age\"]\n      },\n      {\n        id: 3,\n        instruction: \"Exercise-induced symptoms matter too. Add **Exercise Angina** as a predictor.\",\n        target: { variable: \"exang\", zone: \"predictor\" },\n        hint: \"This is a binary variable (yes/no) - GLMs handle these naturally.\",\n        why: \"Patients who experience chest pain during exercise may have lower achievable heart rates.\",\n        highlightVariables: [\"exang\"]\n      },\n      {\n        id: 4,\n        instruction: \"Finally, add **ST Depression** - a measure of heart stress visible on an ECG.\",\n        target: { variable: \"oldpeak\", zone: \"predictor\" },\n        hint: \"This is a continuous measure of cardiac stress during exercise.\",\n        why: \"Higher ST depression values indicate more cardiac stress, which may correlate with heart rate capacity.\",\n        highlightVariables: [\"oldpeak\"]\n      },\n      {\n        id: 5,\n        instruction: \"Excellent! You've specified your first model. Review the equation below, then click **Continue**.\",\n        target: null,\n        hint: null,\n        why: null,\n        highlightVariables: []\n      }\n    ]\n  }\n}\n\n// Tutorial state\nmutable currentStep = 1\nmutable selectedPredictors = []\nmutable selectedResponse = null\n\n// Current step data\ncurrentStepData = tutorialScript.steps[currentStep - 1]\n\n// Check if current step is complete\nstepComplete = {\n  const step = currentStepData\n  if (!step.target) return true\n\n  if (step.target.zone === \"response\") {\n    return selectedResponse?.name === step.target.variable\n  } else {\n    return selectedPredictors.some(p =&gt; p.name === step.target.variable)\n  }\n}\n\n// Available variables (not yet assigned)\navailableVars = {\n  const assigned = new Set([\n    ...selectedPredictors.map(p =&gt; p.name),\n    selectedResponse?.name\n  ].filter(Boolean))\n  return datasetMeta.variables.filter(v =&gt; !assigned.has(v.name))\n}\n\n// Type icons\ntypeIcon = (type) =&gt; {\n  switch(type) {\n    case \"continuous\": return \"ğŸ“Š\"\n    case \"binary\": return \"âš«âšª\"\n    case \"categorical\": return \"ğŸ“‘\"\n    case \"count\": return \"ğŸ”¢\"\n    default: return \"â“\"\n  }\n}"
  },
  {
    "objectID": "pages/tutorial/model-builder.html#learning-objectives",
    "href": "pages/tutorial/model-builder.html#learning-objectives",
    "title": "Tutorial: Building Your First Model",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy completing this tutorial, you will:\n\nUnderstand the difference between response and predictor variables\nLearn to build a model specification visually\nSee how variable choices form a linear predictor equation"
  },
  {
    "objectID": "pages/tutorial/model-builder.html#the-scenario",
    "href": "pages/tutorial/model-builder.html#the-scenario",
    "title": "Tutorial: Building Your First Model",
    "section": "The Scenario",
    "text": "The Scenario\n\n\n\n\n\n\nResearch Question\n\n\n\nWe have cardiovascular data from 303 patients at Cleveland Clinic. We want to understand: What factors affect a patientâ€™s maximum heart rate during exercise?"
  },
  {
    "objectID": "pages/tutorial/model-builder.html#step-currentstep-of-tutorialscript.steps.length",
    "href": "pages/tutorial/model-builder.html#step-currentstep-of-tutorialscript.steps.length",
    "title": "Tutorial: Building Your First Model",
    "section": "Step  of ",
    "text": "Step  of \n\n\nCode\n// Progress bar\nprogressBar = {\n  const progress = ((currentStep - 1) / (tutorialScript.steps.length - 1)) * 100\n  return html`\n    &lt;div class=\"tutorial-progress\"&gt;\n      &lt;div class=\"progress-bar\" style=\"width: ${progress}%\"&gt;&lt;/div&gt;\n      &lt;div class=\"progress-steps\"&gt;\n        ${tutorialScript.steps.map((s, i) =&gt; html`\n          &lt;div class=\"progress-step ${i &lt; currentStep - 1 ? 'completed' : i === currentStep - 1 ? 'current' : 'pending'}\"&gt;\n            ${i &lt; currentStep - 1 ? 'âœ“' : i + 1}\n          &lt;/div&gt;\n        `)}\n      &lt;/div&gt;\n    &lt;/div&gt;\n  `\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\({currentStepData.hint ? html`&lt;p class=\"step-hint\"&gt;&lt;em&gt;\\){currentStepData.hint}\n\n: ''} ${currentStepData.why ? html\n\n\nWhy this variable?\n\n\n\n\n\n` : â€™â€™}\n\n\n\n\nAvailable Variables\n\n\nCode\n// Variable cards with tutorial highlighting\nviewof tutorialVariableCards = {\n  const container = html`&lt;div class=\"variable-pool\"&gt;&lt;/div&gt;`\n\n  function render() {\n    container.innerHTML = ''\n\n    if (availableVars.length === 0) {\n      container.innerHTML = '&lt;p class=\"empty-message\"&gt;All variables assigned&lt;/p&gt;'\n      return\n    }\n\n    const highlightSet = new Set(currentStepData.highlightVariables || [])\n\n    for (const v of availableVars) {\n      const isHighlighted = highlightSet.has(v.name)\n      const isDimmed = highlightSet.size &gt; 0 && !isHighlighted\n\n      const card = html`\n        &lt;div class=\"variable-card ${isHighlighted ? 'tutorial-target' : ''} ${isDimmed ? 'tutorial-dimmed' : ''}\"\n             draggable=\"${!isDimmed}\"\n             data-var=\"${v.name}\"&gt;\n          &lt;div class=\"var-header\"&gt;\n            &lt;span class=\"var-icon\"&gt;${typeIcon(v.type)}&lt;/span&gt;\n            &lt;span class=\"var-name\"&gt;${v.label}&lt;/span&gt;\n          &lt;/div&gt;\n          &lt;div class=\"var-meta\"&gt;\n            &lt;span class=\"var-type\"&gt;${v.type}&lt;/span&gt;\n            ${v.unit ? html`&lt;span class=\"var-unit\"&gt;(${v.unit})&lt;/span&gt;` : ''}\n          &lt;/div&gt;\n          &lt;div class=\"var-desc\"&gt;${v.description}&lt;/div&gt;\n          ${isHighlighted ? html`&lt;div class=\"target-indicator\"&gt;ğŸ‘† Drag me!&lt;/div&gt;` : ''}\n        &lt;/div&gt;\n      `\n\n      if (!isDimmed) {\n        card.addEventListener('dragstart', (e) =&gt; {\n          e.dataTransfer.setData('text/plain', v.name)\n          e.dataTransfer.effectAllowed = 'move'\n          card.classList.add('dragging')\n        })\n\n        card.addEventListener('dragend', () =&gt; {\n          card.classList.remove('dragging')\n        })\n      }\n\n      container.appendChild(card)\n    }\n  }\n\n  render()\n  return container\n}\n\n\n\n\n\n\n\n\n\n\nModel Canvas\n\n\nCode\nisValidDrop = (varName, zone) =&gt; {\n  const step = currentStepData\n  if (!step.target) return true\n  return step.target.variable === varName && step.target.zone === zone\n}\n\n// Predictor drop zone\nviewof tutorialPredictorZone = {\n  const isTarget = currentStepData.target?.zone === \"predictor\"\n\n  const zone = html`\n    &lt;div class=\"drop-zone predictor-zone ${isTarget ? 'tutorial-target-zone' : ''}\"&gt;\n      &lt;h4&gt;Predictors (X)&lt;/h4&gt;\n      &lt;p class=\"zone-hint\"&gt;Variables that might explain the response&lt;/p&gt;\n      &lt;div class=\"assigned-vars\" id=\"predictor-list\"&gt;&lt;/div&gt;\n    &lt;/div&gt;\n  `\n\n  const list = zone.querySelector('#predictor-list')\n\n  function renderPredictors() {\n    list.innerHTML = ''\n\n    if (selectedPredictors.length === 0) {\n      list.innerHTML = '&lt;p class=\"empty-hint\"&gt;Drop predictor variables here&lt;/p&gt;'\n      return\n    }\n\n    selectedPredictors.forEach((v, i) =&gt; {\n      const chip = html`\n        &lt;div class=\"assigned-chip predictor-chip\"&gt;\n          &lt;span class=\"chip-label\"&gt;X&lt;sub&gt;${i + 1}&lt;/sub&gt;&lt;/span&gt;\n          &lt;span class=\"chip-icon\"&gt;${typeIcon(v.type)}&lt;/span&gt;\n          &lt;span class=\"chip-name\"&gt;${v.label}&lt;/span&gt;\n        &lt;/div&gt;\n      `\n      list.appendChild(chip)\n    })\n  }\n\n  zone.addEventListener('dragover', (e) =&gt; {\n    e.preventDefault()\n    zone.classList.add('drag-over')\n  })\n\n  zone.addEventListener('dragleave', () =&gt; {\n    zone.classList.remove('drag-over')\n  })\n\n  zone.addEventListener('drop', (e) =&gt; {\n    e.preventDefault()\n    zone.classList.remove('drag-over')\n    const varName = e.dataTransfer.getData('text/plain')\n\n    if (!isValidDrop(varName, 'predictor')) {\n      zone.classList.add('tutorial-wrong')\n      setTimeout(() =&gt; zone.classList.remove('tutorial-wrong'), 500)\n      return\n    }\n\n    const variable = datasetMeta.variables.find(v =&gt; v.name === varName)\n    if (variable && !selectedPredictors.find(p =&gt; p.name === varName)) {\n      mutable selectedPredictors = [...selectedPredictors, variable]\n\n      // Advance tutorial step after short delay\n      setTimeout(() =&gt; {\n        if (currentStep &lt; tutorialScript.steps.length) {\n          mutable currentStep = currentStep + 1\n        }\n      }, 600)\n\n      renderPredictors()\n    }\n  })\n\n  renderPredictors()\n  return zone\n}\n\n// Response drop zone\nviewof tutorialResponseZone = {\n  const isTarget = currentStepData.target?.zone === \"response\"\n\n  const zone = html`\n    &lt;div class=\"drop-zone response-zone ${isTarget ? 'tutorial-target-zone' : ''}\"&gt;\n      &lt;h4&gt;Response (y)&lt;/h4&gt;\n      &lt;p class=\"zone-hint\"&gt;The outcome you want to predict&lt;/p&gt;\n      &lt;div class=\"assigned-vars\" id=\"response-slot\"&gt;&lt;/div&gt;\n    &lt;/div&gt;\n  `\n\n  const slot = zone.querySelector('#response-slot')\n\n  function renderResponse() {\n    slot.innerHTML = ''\n\n    if (!selectedResponse) {\n      slot.innerHTML = '&lt;p class=\"empty-hint\"&gt;Drop response variable here&lt;/p&gt;'\n      return\n    }\n\n    const chip = html`\n      &lt;div class=\"assigned-chip response-chip\"&gt;\n        &lt;span class=\"chip-label\"&gt;y&lt;/span&gt;\n        &lt;span class=\"chip-icon\"&gt;${typeIcon(selectedResponse.type)}&lt;/span&gt;\n        &lt;span class=\"chip-name\"&gt;${selectedResponse.label}&lt;/span&gt;\n      &lt;/div&gt;\n    `\n    slot.appendChild(chip)\n  }\n\n  zone.addEventListener('dragover', (e) =&gt; {\n    e.preventDefault()\n    zone.classList.add('drag-over')\n  })\n\n  zone.addEventListener('dragleave', () =&gt; {\n    zone.classList.remove('drag-over')\n  })\n\n  zone.addEventListener('drop', (e) =&gt; {\n    e.preventDefault()\n    zone.classList.remove('drag-over')\n    const varName = e.dataTransfer.getData('text/plain')\n\n    if (!isValidDrop(varName, 'response')) {\n      zone.classList.add('tutorial-wrong')\n      setTimeout(() =&gt; zone.classList.remove('tutorial-wrong'), 500)\n      return\n    }\n\n    const variable = datasetMeta.variables.find(v =&gt; v.name === varName)\n    if (variable) {\n      mutable selectedResponse = variable\n\n      // Advance tutorial step after short delay\n      setTimeout(() =&gt; {\n        if (currentStep &lt; tutorialScript.steps.length) {\n          mutable currentStep = currentStep + 1\n        }\n      }, 600)\n\n      renderResponse()\n    }\n  })\n\n  renderResponse()\n  return zone\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour Model So Far\n\n\nCode\ntutorialModelSummary = {\n  if (!selectedResponse) {\n    return html`&lt;p class=\"model-building\"&gt;Waiting for you to select a response variable...&lt;/p&gt;`\n  }\n\n  if (selectedPredictors.length === 0) {\n    return html`\n      &lt;div class=\"model-equation partial\"&gt;\n        &lt;p&gt;&lt;strong&gt;Response:&lt;/strong&gt; ${selectedResponse.label}&lt;/p&gt;\n        &lt;p class=\"model-building\"&gt;Add predictors to build the linear predictor equation...&lt;/p&gt;\n      &lt;/div&gt;\n    `\n  }\n\n  const predictorTerms = selectedPredictors.map((p, i) =&gt;\n    `Î²&lt;sub&gt;${i+1}&lt;/sub&gt;Â·${p.label.replace(/ /g, '')}`\n  ).join(' + ')\n\n  return html`\n    &lt;div class=\"model-equation ${currentStep === tutorialScript.steps.length ? 'complete' : ''}\"&gt;\n      &lt;p&gt;&lt;strong&gt;Response (y):&lt;/strong&gt; ${selectedResponse.label}&lt;/p&gt;\n      &lt;p&gt;&lt;strong&gt;Predictors (X):&lt;/strong&gt; ${selectedPredictors.map(p =&gt; p.label).join(', ')}&lt;/p&gt;\n      &lt;p&gt;&lt;strong&gt;Linear predictor:&lt;/strong&gt;&lt;/p&gt;\n      &lt;p class=\"equation\"&gt;Î· = Î²&lt;sub&gt;0&lt;/sub&gt; + ${predictorTerms}&lt;/p&gt;\n      ${currentStep === tutorialScript.steps.length ? html`\n        &lt;p class=\"model-complete\"&gt;âœ“ Model specification complete!&lt;/p&gt;\n      ` : ''}\n    &lt;/div&gt;\n  `\n}\n\n\n\n\n\n\n\n\n\n\n\nCode\n// Navigation\ntutorialNav = {\n  const canContinue = currentStep === tutorialScript.steps.length\n\n  return html`\n    &lt;div class=\"nav-buttons\"&gt;\n      &lt;button class=\"btn btn-secondary\" onclick=${() =&gt; {\n        mutable currentStep = 1\n        mutable selectedPredictors = []\n        mutable selectedResponse = null\n      }}&gt;â†» Restart Tutorial&lt;/button&gt;\n\n      &lt;div class=\"nav-right\"&gt;\n        &lt;a href=\"../model-builder.html\" class=\"btn btn-outline\"&gt;Skip to Free Explore&lt;/a&gt;\n        &lt;button class=\"btn btn-primary\" ${canContinue ? '' : 'disabled'}&gt;\n          Continue to Transforms â†’\n        &lt;/button&gt;\n      &lt;/div&gt;\n    &lt;/div&gt;\n  `\n}"
  },
  {
    "objectID": "pages/tutorial/model-builder.html#what-youre-learning",
    "href": "pages/tutorial/model-builder.html#what-youre-learning",
    "title": "Tutorial: Building Your First Model",
    "section": "What Youâ€™re Learning",
    "text": "What Youâ€™re Learning\n\n\n\n\n\n\nUnderstanding the Model Structure\n\n\n\n\n\nWhen you select variables, youâ€™re defining:\n\nResponse (y): The variable we observe and want to explain\nPredictors (X): Variables we think influence the response\nLinear predictor (Î·): A weighted sum of predictors: Î· = Î²â‚€ + Î²â‚Xâ‚ + Î²â‚‚Xâ‚‚ + â€¦\n\nThe Î² coefficients (which weâ€™ll estimate later) tell us how much each predictor affects the response."
  },
  {
    "objectID": "pages/model-builder.html#dataset-heart-disease",
    "href": "pages/model-builder.html#dataset-heart-disease",
    "title": "Build Your Model",
    "section": "Dataset: Heart Disease",
    "text": "Dataset: Heart Disease\nCardiovascular health indicators from patients at Cleveland Clinic\n\n\n\n\n\n\nHow to use this page\n\n\n\n\nDrag variables from the table below to assign them as predictors (left) or response (right)\nYou need at least one predictor and exactly one response to continue\nClick the âœ• on any assigned variable to return it to the pool\n\n\n\n\nAvailable Variables\nDrag variables from here to the model canvas below.\n\n\nCode\n// Variable cards - reactive to availableVars\nvariablePool = {\n  const container = document.createElement('div')\n  container.className = 'variable-pool'\n\n  if (availableVars.length === 0) {\n    container.innerHTML = '&lt;p class=\"empty-message\"&gt;All variables assigned to model&lt;/p&gt;'\n    return container\n  }\n\n  for (const v of availableVars) {\n    const card = document.createElement('div')\n    card.className = 'variable-card'\n    card.draggable = true\n    card.dataset.var = v.name\n\n    card.innerHTML = `\n      &lt;div class=\"var-header\"&gt;\n        &lt;span class=\"var-icon\"&gt;${typeIcon(v.type)}&lt;/span&gt;\n        &lt;span class=\"var-name\"&gt;${v.label}&lt;/span&gt;\n      &lt;/div&gt;\n      &lt;div class=\"var-meta\"&gt;\n        &lt;span class=\"var-type\"&gt;${v.type}&lt;/span&gt;\n        ${v.unit ? `&lt;span class=\"var-unit\"&gt;(${v.unit})&lt;/span&gt;` : ''}\n      &lt;/div&gt;\n      &lt;div class=\"var-desc\"&gt;${v.description}&lt;/div&gt;\n    `\n\n    card.addEventListener('dragstart', (e) =&gt; {\n      e.dataTransfer.setData('text/plain', v.name)\n      e.dataTransfer.effectAllowed = 'move'\n      card.classList.add('dragging')\n    })\n\n    card.addEventListener('dragend', () =&gt; {\n      card.classList.remove('dragging')\n    })\n\n    container.appendChild(card)\n  }\n\n  return container\n}\n\n\n\n\n\n\n\n\n\n\nModel Canvas\n\n\n\n\nCode\n// Predictor drop zone - reactive\npredictorZone = {\n  const zone = document.createElement('div')\n  zone.className = 'drop-zone predictor-zone'\n\n  // Render content\n  let content = `\n    &lt;h4&gt;Predictors (X)&lt;/h4&gt;\n    &lt;p class=\"zone-hint\"&gt;Variables that might explain or predict the response&lt;/p&gt;\n    &lt;div class=\"assigned-vars\"&gt;\n  `\n\n  if (selectedPredictors.length === 0) {\n    content += '&lt;p class=\"empty-hint\"&gt;Drop variables here&lt;/p&gt;'\n  } else {\n    selectedPredictors.forEach((v, i) =&gt; {\n      content += `\n        &lt;div class=\"assigned-chip predictor-chip\" data-var=\"${v.name}\"&gt;\n          &lt;span class=\"chip-label\"&gt;X&lt;sub&gt;${i + 1}&lt;/sub&gt;&lt;/span&gt;\n          &lt;span class=\"chip-icon\"&gt;${typeIcon(v.type)}&lt;/span&gt;\n          &lt;span class=\"chip-name\"&gt;${v.label}&lt;/span&gt;\n          &lt;button class=\"chip-remove\" data-var=\"${v.name}\"&gt;âœ•&lt;/button&gt;\n        &lt;/div&gt;\n      `\n    })\n  }\n\n  content += '&lt;/div&gt;'\n  zone.innerHTML = content\n\n  // Add remove handlers\n  zone.querySelectorAll('.chip-remove').forEach(btn =&gt; {\n    btn.addEventListener('click', (e) =&gt; {\n      const varName = e.target.dataset.var\n      mutable selectedPredictors = selectedPredictors.filter(p =&gt; p.name !== varName)\n    })\n  })\n\n  // Drop handlers\n  zone.addEventListener('dragover', (e) =&gt; {\n    e.preventDefault()\n    e.dataTransfer.dropEffect = 'move'\n    zone.classList.add('drag-over')\n  })\n\n  zone.addEventListener('dragleave', () =&gt; {\n    zone.classList.remove('drag-over')\n  })\n\n  zone.addEventListener('drop', (e) =&gt; {\n    e.preventDefault()\n    zone.classList.remove('drag-over')\n    const varName = e.dataTransfer.getData('text/plain')\n    const variable = datasetMeta.variables.find(v =&gt; v.name === varName)\n\n    if (variable && !selectedPredictors.find(p =&gt; p.name === varName)) {\n      // If it was the response, remove it from there\n      if (selectedResponse?.name === varName) {\n        mutable selectedResponse = null\n      }\n      mutable selectedPredictors = [...selectedPredictors, variable]\n    }\n  })\n\n  return zone\n}\n\n\n\n\n\n\n\n\n\n\n\nCode\n// Response drop zone - reactive\nresponseZone = {\n  const zone = document.createElement('div')\n  zone.className = 'drop-zone response-zone'\n\n  // Render content\n  let content = `\n    &lt;h4&gt;Response (y)&lt;/h4&gt;\n    &lt;p class=\"zone-hint\"&gt;The outcome variable you want to model&lt;/p&gt;\n    &lt;div class=\"assigned-vars\"&gt;\n  `\n\n  if (!selectedResponse) {\n    content += '&lt;p class=\"empty-hint\"&gt;Drop one variable here&lt;/p&gt;'\n  } else {\n    content += `\n      &lt;div class=\"assigned-chip response-chip\"&gt;\n        &lt;span class=\"chip-label\"&gt;y&lt;/span&gt;\n        &lt;span class=\"chip-icon\"&gt;${typeIcon(selectedResponse.type)}&lt;/span&gt;\n        &lt;span class=\"chip-name\"&gt;${selectedResponse.label}&lt;/span&gt;\n        &lt;button class=\"chip-remove\"&gt;âœ•&lt;/button&gt;\n      &lt;/div&gt;\n    `\n  }\n\n  content += '&lt;/div&gt;'\n  zone.innerHTML = content\n\n  // Add remove handler\n  const removeBtn = zone.querySelector('.chip-remove')\n  if (removeBtn) {\n    removeBtn.addEventListener('click', () =&gt; {\n      mutable selectedResponse = null\n    })\n  }\n\n  // Drop handlers\n  zone.addEventListener('dragover', (e) =&gt; {\n    e.preventDefault()\n    e.dataTransfer.dropEffect = 'move'\n    zone.classList.add('drag-over')\n  })\n\n  zone.addEventListener('dragleave', () =&gt; {\n    zone.classList.remove('drag-over')\n  })\n\n  zone.addEventListener('drop', (e) =&gt; {\n    e.preventDefault()\n    zone.classList.remove('drag-over')\n    const varName = e.dataTransfer.getData('text/plain')\n    const variable = datasetMeta.variables.find(v =&gt; v.name === varName)\n\n    if (variable) {\n      // Remove from predictors if it was there\n      mutable selectedPredictors = selectedPredictors.filter(p =&gt; p.name !== varName)\n      mutable selectedResponse = variable\n    }\n  })\n\n  return zone\n}\n\n\n\n\n\n\n\n\n\n\n\n\nCurrent Model Specification\n\n\nCode\nmodelSummary = {\n  const div = document.createElement('div')\n\n  if (!selectedResponse) {\n    div.className = 'model-incomplete'\n    div.innerHTML = 'âš ï¸ Select a response variable (y) to define your model'\n    return div\n  }\n\n  if (selectedPredictors.length === 0) {\n    div.className = 'model-incomplete'\n    div.innerHTML = 'âš ï¸ Select at least one predictor variable (X)'\n    return div\n  }\n\n  const predictorTerms = selectedPredictors\n    .map((p, i) =&gt; `Î²&lt;sub&gt;${i+1}&lt;/sub&gt;Â·${p.label}`)\n    .join(' + ')\n\n  const familyNote = selectedResponse.type === 'continuous'\n    ? 'â†’ Gaussian family with identity link is a natural starting point'\n    : selectedResponse.type === 'binary'\n      ? 'â†’ Binomial family with logit link is appropriate'\n      : selectedResponse.type === 'count'\n        ? 'â†’ Poisson family with log link is a starting point'\n        : 'â†’ Consider the response type when choosing distribution family'\n\n  div.className = 'model-equation'\n  div.innerHTML = `\n    &lt;p&gt;&lt;strong&gt;Response:&lt;/strong&gt; ${selectedResponse.label} (${selectedResponse.type})&lt;/p&gt;\n    &lt;p&gt;&lt;strong&gt;Linear predictor:&lt;/strong&gt;&lt;/p&gt;\n    &lt;p class=\"equation\"&gt;Î· = Î²&lt;sub&gt;0&lt;/sub&gt; + ${predictorTerms}&lt;/p&gt;\n    &lt;p class=\"model-note\"&gt;${familyNote}&lt;/p&gt;\n  `\n\n  return div\n}\n\n\n\n\n\n\n\n\n\nCode\n// Navigation\ncanProceed = selectedResponse !== null && selectedPredictors.length &gt; 0\n\n\n\n\n\n\n\n\n\nCode\nnavButtons = {\n  const div = document.createElement('div')\n  div.className = 'nav-buttons'\n  div.innerHTML = `\n    &lt;a href=\"index.html\" class=\"btn btn-secondary\"&gt;â† Back to Scenarios&lt;/a&gt;\n    &lt;button class=\"btn btn-primary\" ${canProceed ? '' : 'disabled'}&gt;\n      Continue to Model Selection â†’\n    &lt;/button&gt;\n  `\n  return div\n}\n\n\n\n\n\n\n\n\n\n\nSample Data Preview\n\n\nCode\nInputs.table(datasetMeta.sampleData, {\n  columns: datasetMeta.variables.map(v =&gt; v.name),\n  header: Object.fromEntries(datasetMeta.variables.map(v =&gt; [v.name, v.label])),\n  width: {\n    age: 60,\n    sex: 50,\n    cp: 50,\n    trestbps: 80,\n    chol: 70,\n    fbs: 50,\n    restecg: 70,\n    thalach: 80,\n    exang: 60,\n    oldpeak: 70,\n    slope: 60,\n    ca: 50,\n    thal: 50,\n    num: 50\n  },\n  rows: 8\n})"
  },
  {
    "objectID": "claude.html#tutorial-series-current-focus",
    "href": "claude.html#tutorial-series-current-focus",
    "title": "",
    "section": "Tutorial Series (Current Focus)",
    "text": "Tutorial Series (Current Focus)\nA series of guided tutorials, each demonstrating a complete GLM workflow for different data types. See .claude/specs/tutorial-series.md for detailed implementation plan.\n\nCurrent Status\n\n\n\n#\nTutorial\nGLM Family\nStatus\n\n\n\n\n1\nHeart Rate Prediction\nGaussian + Identity\nCOMPLETE\n\n\n2\nHeart Disease Classification\nBinomial + Logit\nCOMPLETE\n\n\n3\nBike Rental Counts\nPoisson + Log\nCOMPLETE\n\n\n4\nOverdispersed Counts\nNegative Binomial + Log\nCOMPLETE\n\n\n5\nInsurance Claim Amounts\nGamma + Log\nOPTIONAL\n\n\n\n\n\nTutorial Files\nprototype/\nâ”œâ”€â”€ index.html                      # Tutorial/datasets index page\nâ””â”€â”€ tutorials/\n    â”œâ”€â”€ 01-gaussian/                # Tutorial 1: Heart Rate (Gaussian)\n    â”‚   â”œâ”€â”€ systematic.html         # Select response & predictors\n    â”‚   â”œâ”€â”€ link.html               # Link function selection\n    â”‚   â”œâ”€â”€ distribution.html       # Distribution selection\n    â”‚   â”œâ”€â”€ fitting.html            # Fitting method explanation\n    â”‚   â”œâ”€â”€ code.html               # R/Python implementation\n    â”‚   â””â”€â”€ advanced.html           # Log-likelihood derivation (KaTeX)\n    â”œâ”€â”€ 02-logistic/                # Tutorial 2: Heart Disease (Binomial)\n    â”œâ”€â”€ 03-poisson/                 # Tutorial 3: Bike Rentals (Poisson)\n    â”œâ”€â”€ 04-negbin/                  # Tutorial 4: Overdispersed Counts\n    â””â”€â”€ 05-gamma/                   # Tutorial 5: Insurance Claims (optional)\n\n\nDesign Pattern\nEach tutorial follows a 6-page flow: 1. Systematic â†’ Select response & predictors 2. Link â†’ Choose link function (with wrong-choice feedback) 3. Distribution â†’ Choose distribution family 4. Fitting â†’ Understand MLE/IRLS 5. Code â†’ R and Python implementation 6. Advanced â†’ Log-likelihood derivation, custom MLE\n\n\nKey Technical Decisions\n\nKaTeX for LaTeX math rendering (CDN-based)\nStandalone HTML prototypes (no Quarto yet)\nTabbed R/Python code panels\nValidated outputs against actual R/Python sessions"
  }
]